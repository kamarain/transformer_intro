\documentclass[final]{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
\usepackage[nonatbib]{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

% Own packages
\usepackage{listings}
\usepackage[
    backend=biber,
    %style=authoryear-icomp,
    %sortlocale=de_DE,
    natbib=true,
    %url=false, 
    %doi=true,
    %eprint=false
]{biblatex}
\addbibresource{transformers.bib}


\title{Formatting Instructions For NeurIPS 2024}
\title{Introduction to Sequence Modeling with Transformers}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Joni-Kristian K{\"a}m{\"a}r{\"a}inen\thanks{See \url{https://webpages.tuni.fi/vision/public_pages/JoniKamarainen/}}\\
  Department of Computing Sciences\\
  Tampere University
  %\texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

%\author{%
%  David S.~Hippocampus\thanks{Use footnote for providing further information
%    about author (webpage, alternative address)---\emph{not} for acknowledging
%    funding agencies.} \\
%  Department of Computer Science\\
%  Cranberry-Lemon University\\
%  Pittsburgh, PA 15213 \\
%  \texttt{hippo@cs.cranberry-lemon.edu} \\
%  % examples of more authors
%  % \And
%  % Coauthor \\
%  % Affiliation \\
%  % Address \\
%  % \texttt{email} \\
%  % \AND
%  % Coauthor \\
%  % Affiliation \\
%  % Address \\
%  % \texttt{email} \\
%  % \And
%  % Coauthor \\
%  % Affiliation \\
%  % Address \\
%  % \texttt{email} \\
%  % \And
%  % Coauthor \\
%  % Affiliation \\
%  % Address \\
%  % \texttt{email} \\
%}


%% MY DEFINITIONS
%\lstdefinestyle{mystyle}{
%  belowcaptionskip=1\baselineskip,
%  breaklines=true,
%  frame=single,
%  xleftmargin=\parindent,
%  language=Python,
%  showstringspaces=false,
%  basicstyle=\ttfamily,
%  keywordstyle=\bfseries\color{green!40!black},
%  commentstyle=\itshape\color{purple!40!black},
%  identifierstyle=\color{blue},
%  stringstyle=\color{orange},
%  moredelim=**[is][\color{red}]{@}{@},
%}
%
%\lstset{language=Python,style=mystyle} 
\lstset{language=Python, basicstyle=\scriptsize} 


\begin{document}


\maketitle


\begin{abstract}
  Understanding the transformer architecture and its workings is
  essential for machine learning (ML) engineers. However, truly
  understanding the transformer architecture can be demanding, even if
  you have a solid background in machine learning or deep
  learning. The main working horse is attention, which yields to the
  transformer encoder-decoder structure. However, putting attention
  aside leaves several programming components that are easy to
  implement but whose role for the whole is unclear. These components
  are 'tokenization', 'embedding' ('un-embedding'), 'masking', and
  'positional encoding'. The focus of this work is on understanding
  them. To keep things simple, the understanding is built
  incrementally by adding components one by one, and after each step
  investigating what is doable and what is undoable with the current
  model. Simple sequences of zeros (0) and ones (1) are used to study
  the workings of each step.
\end{abstract}

\section{Background}
If you are familiar with \textit{machine learning} and
\textit{sequence modeling}, then you can skip this section. This
section covers the terminology and concepts used in the rest of the work.

\paragraph{Supervised machine learning.}
The core supervised machine learning prolem is to find a model $f$ that
maps inputs $x$ to outputs $y$
\begin{equation}
  y = f(x) \enspace .
  \label{eq:supervisedML}
\end{equation}
Before the recent more powerful models of deep learning,
the best neural model for supervised ML was the Multilayer Perception
(MLP) neural network. An excellent introduction to MLPs and other
methods of conventional machine learning is the Bishop's book from
2006~\cite{MLBook}.

In the deep learning vocabulary, MLP is a
stack of \textit{fully connected layers} and \textit{non-linear
  activation functions} connecting the layers. MLP is trained by adjusting
the model parameters, weights $\theta$, to minimize some error
(\textit{loss function}). The standard loss is
Mean Squared Error (MSE)
\[
\mathcal{L}_{MSE} = \sum_{i=1}^{K} (y_i-f_\theta (x))^2 \enspace .
\]
The MSE loss is computed for $K$ input-output pairs (\textit{training
  data}). More formally, the optimization
problem is
\[
\theta = \arg\min_{\theta} \mathcal{L}_{MSE} \enspace ,
\]
which can be iteratively optimized with the gradient descent
algorithms (\textit{backpropagation}).

A Torch-style code block for two-layer MLP network is 
\begin{lstlisting}
.init(self, input_dim, hidden_size, output_dim):
        self.dense1 = torch.nn.Linear(input_dim, hidden_size)
        self.dense2 = torch.nn.Linear(hidden_size, hidden_size)
        self.output = torch.nn.Linear(hidden_size, output_dim)

.forward(self, x):
    x = self.dense1(x)
    x = torch.nn.functional.sigmoid(x)
    x = self.dense2(x)
    x = torch.nn.functional.sigmoid(x)
    y_pred = self.output(x)
    return y_pred
\end{lstlisting}


\paragraph{Sequence modeling.}
A common example of sequence modeling
is the problem of time series forecasting. An application of time
series forecasting is stock price prediction. 
The problem is to estimate the future values
$\hat{x}_N, \hat{x}_{N+1}, \ldots, \hat{x}_{N+M}$ of a process
from which some past values $x_0, x_{1}, \ldots, x_{N-1}$ are known.
The hat symbol $\hat{\cdot}$ is used to distinguish between the true
(ground-truth) and predicted values.

In the ML formalism (\ref{eq:supervisedML}) the sequence
modeling problem can be defined as
\[
\mathbf{Y} = f_\theta (\mathbf{X}) .
\]
However, the problem is that the size of $\mathbf{X}$ and $\mathbf{Y}$
can vary but the network structure parameter 
\texttt{input\_dim} and \texttt{output\_dim} must be predefined
(Transformers do not have this limitation). An alternative approach is
to use the previous element to predict the next sequence element
\[
x_{n+1} = f_\theta (x_{n}) .
\]
However, this na√Øve approach does not work very well. For example, it
cannot model sinusoidal signals since the same point (e.g. zero) is
'visited' on the both falling and raising edges of the sinusoidal. One
option is to add multiple previous outputs, but there are better
neural models than that. 

\paragraph{Sequence-to-sequence (Seq2Seq) modeling.}
In the neural networks literature, Seq2Seq models are related to
Recurrent Neural Networks (RNNs). RNNs maintain a system 'state' $h_t$
that contain information about 'what has been produced so far'. Again,
in the MLP terms this is a model of the type
\[
x_{n+1},h_{n+1} = f_\theta (x_{n},h_{n+1})
\]
and perhaps the best known such structure is the Long Short-Term
Memory (LSTM)~\cite{LSTM}. However, the original paper is rather
tedious to follow. Instead, one should read the papers related to
neural translation that progressed from RNNs to the transformer
architecture.

\citet{Sutskever-2014-neurips} is the best starting point to
understand the ideas behind Transformer. They define the generic idea
of machine translation where two RNNs form an encoder-decored
structure. They use LSTM as the RNN model, but the architecture is
general. The first LSTM (encoder) encodes the input sequence of $N$
samples which is encoded into the last system state $h_{N}$. We assume
that $h_0$ is the initial state, 'Start of Sequence' (SOS), and also after
the first sample there can be similar 'End of Sequence' (EOS) state
vector. The final encoder state is then used as the first state vector
for the decoder in addition to the SOS vector. Output is read from the
decoder until EOS appears.

The same structure was used in~\cite{Cho-2014-emnlp}, but they also
notified that such structure quickly 'forgets' past samples despite
that infinite memory is theoretically possible. They introduced
'attention' as mechanism to overcome this limitation. In their system
all input states $h_i$ are stored, an attention weight is computed for
all of them, and they are combined before delivering to the
decoder. This attention mechanism is ofter referred to
\textit{Bahdanau attention}.

Finally, the Transformer architecture was introduced
in~\cite{transformer}. Its main motivation is to avoid the sequential
processing of RNNs which makes optimization slow. Instead, they
compute attention between the input sequence samples at the same time
using matrix operations for which automatic differentation provides
gradient.

\section{Methods}


\begin{ack}
This work would not be possible without tenure in Tampere
University. My tenured position allows me to allocate time to projects
that benefit society at large, or benefit nobody if they ideas just
fail. I hope that you cite this work if you find it useful since that
helps me to justify the time and efforts used for this writing this
article and preparing the code examples.

Universities are beautiful
places and research, teaching, and supervising is the best I know, but
I am afraid that blind believe in academic metrics and the corporate
style management of universities will destroy these holy places of
academic work.
\end{ack}

\printbibliography

\end{document}
