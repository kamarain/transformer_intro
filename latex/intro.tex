\documentclass[final]{article}


% if you need to pass options to natbib, use, e.g.:
%     \PassOptionsToPackage{numbers, compress}{natbib}
% before loading neurips_2024


% ready for submission
\usepackage[nonatbib]{neurips_2024}


% to compile a preprint version, e.g., for submission to arXiv, add add the
% [preprint] option:
%     \usepackage[preprint]{neurips_2024}


% to compile a camera-ready version, add the [final] option, e.g.:
%     \usepackage[final]{neurips_2024}


% to avoid loading the natbib package, add option nonatbib:
%    \usepackage[nonatbib]{neurips_2024}


\usepackage[utf8]{inputenc} % allow utf-8 input
\usepackage[T1]{fontenc}    % use 8-bit T1 fonts
\usepackage{hyperref}       % hyperlinks
\usepackage{url}            % simple URL typesetting
\usepackage{booktabs}       % professional-quality tables
\usepackage{amsfonts}       % blackboard math symbols
\usepackage{nicefrac}       % compact symbols for 1/2, etc.
\usepackage{microtype}      % microtypography
\usepackage{xcolor}         % colors

% Own packages
\usepackage{listings}
\usepackage[
    backend=biber,
    %style=authoryear-icomp,
    %sortlocale=de_DE,
    natbib=true,
    %url=false, 
    %doi=true,
    %eprint=false
]{biblatex}
\addbibresource{transformers.bib}


\title{Formatting Instructions For NeurIPS 2024}
\title{Introduction to Sequence Modeling with Transformers}


% The \author macro works with any number of authors. There are two commands
% used to separate the names and addresses of multiple authors: \And and \AND.
%
% Using \And between authors leaves it to LaTeX to determine where to break the
% lines. Using \AND forces a line break at that point. So, if LaTeX puts 3 of 4
% authors names on the first line, and the last on the second line, try using
% \AND instead of \And before the third author name.


\author{%
  Joni-Kristian K{\"a}m{\"a}r{\"a}inen\thanks{See \url{https://webpages.tuni.fi/vision/public_pages/JoniKamarainen/}}\\
  Department of Computing Sciences\\
  Tampere University
  %\texttt{hippo@cs.cranberry-lemon.edu} \\
  % examples of more authors
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \AND
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
  % \And
  % Coauthor \\
  % Affiliation \\
  % Address \\
  % \texttt{email} \\
}

%\author{%
%  David S.~Hippocampus\thanks{Use footnote for providing further information
%    about author (webpage, alternative address)---\emph{not} for acknowledging
%    funding agencies.} \\
%  Department of Computer Science\\
%  Cranberry-Lemon University\\
%  Pittsburgh, PA 15213 \\
%  \texttt{hippo@cs.cranberry-lemon.edu} \\
%  % examples of more authors
%  % \And
%  % Coauthor \\
%  % Affiliation \\
%  % Address \\
%  % \texttt{email} \\
%  % \AND
%  % Coauthor \\
%  % Affiliation \\
%  % Address \\
%  % \texttt{email} \\
%  % \And
%  % Coauthor \\
%  % Affiliation \\
%  % Address \\
%  % \texttt{email} \\
%  % \And
%  % Coauthor \\
%  % Affiliation \\
%  % Address \\
%  % \texttt{email} \\
%}


%% MY DEFINITIONS
%\lstdefinestyle{mystyle}{
%  belowcaptionskip=1\baselineskip,
%  breaklines=true,
%  frame=single,
%  xleftmargin=\parindent,
%  language=Python,
%  showstringspaces=false,
%  basicstyle=\ttfamily,
%  keywordstyle=\bfseries\color{green!40!black},
%  commentstyle=\itshape\color{purple!40!black},
%  identifierstyle=\color{blue},
%  stringstyle=\color{orange},
%  moredelim=**[is][\color{red}]{@}{@},
%}
%
%\lstset{language=Python,style=mystyle} 
\lstset{language=Python, basicstyle=\scriptsize} 


\begin{document}


\maketitle


\begin{abstract}
  Understanding the transformer architecture and its workings is
  essential for machine learning (ML) engineers. However, truly
  understanding the transformer architecture can be demanding, even if
  you have a solid background in machine learning or deep
  learning. The main working horse is attention, which yields to the
  transformer encoder-decoder structure. However, putting attention
  aside leaves several programming components that are easy to
  implement but whose role for the whole is unclear. These components
  are 'tokenization', 'embedding' ('un-embedding'), 'masking', and
  'positional encoding'. The focus of this work is on understanding
  them. To keep things simple, the understanding is built
  incrementally by adding components one by one, and after each step
  investigating what is doable and what is undoable with the current
  model. Simple sequences of zeros (0) and ones (1) are used to study
  the workings of each step.
\end{abstract}

\section{Background}
If you already know \textit{machine learning} and
\textit{sequence modeling} well, then this section can be skipped. The
purpose of this section is to 
cover the terminology and concepts used in the work.

\paragraph{Supervised machine learning.}
The supervised machine learning problem is to find a model $f$ that
maps inputs $x$ to outputs $y$
\begin{equation}
  y = f(x) \enspace .
  \label{eq:supervisedML}
\end{equation}
The conventional methods for the ML problem are regression and classification.
Before the recent deep learning methods,
the most popular neural ML model was Multilayer Perceptron (MLP). The
fundamental ideas and methods of conventional ML including MLPs are
well covered in Bishop's ML book from 2006~\cite{MLBook}
(available online).

Using the deep learning terminology, MLP is a neural network of
\textit{fully connected layers} and \textit{non-linear activation
  functions} (such as ReLU or Sigmoid) between them. Through the
layers, MLP maps inputs $x$ to outputs $y$. Each neuron (\textit{unit}
or \textit{kernel}) has weights which are the learnable
parameters $\theta$ of the network. Network training is based on
adjusting the weights so that some error (\textit{loss function})
is minimized. The typical loss function for regression is the Mean
Squared Error (MSE) and for classification Cross Entropy (CE). The MSE
loss is defined
\[
\mathcal{L}_{MSE} = \sum_{i=1}^{K} (y_i-f_\theta (x))^2 \enspace .
\]


The loss is computed for a set of $K$ input-output pairs, which is
the \textit{training data}. Formally, the optimization is defined
as
\[
\theta = \arg\min_{\theta} \mathcal{L}_{MSE} \enspace ,
\]
and the standard technique for optimization is the Stochastic Gradient
Descent (SGD) algorithm. The process of passing the loss gradient
through the network is called \textit{error backpropagation}.

A Torch-style code block of a two-layer MLP network:
\begin{lstlisting}
.init(self, input_dim, hidden_size, output_dim):
        self.dense1 = torch.nn.Linear(input_dim, hidden_size)
        self.dense2 = torch.nn.Linear(hidden_size, hidden_size)
        self.output = torch.nn.Linear(hidden_size, output_dim)

.forward(self, x):
    x = self.dense1(x)
    x = torch.nn.functional.sigmoid(x)
    x = self.dense2(x)
    x = torch.nn.functional.sigmoid(x)
    y_pred = self.output(x)
    return y_pred
\end{lstlisting}



\paragraph{Sequence modeling.}
A problem domain where sequence modeling is used is time series
forecasting. For example, forecasting future stock prices is a time
series forecasting application. The problem can be defined as the
problem of predicting the future values
$\hat{x}_N, \hat{x}_{N+1}, \ldots, \hat{x}_{N+M}$ of a process
from which past values $x_0, x_{1}, \ldots, x_{N-1}$ are known.
The hat symbol $\hat{\cdot}$ is used to distinguish between the true
(ground truth) and predicted values.

An ML-compliant definition is to find the model $f_\theta$ so that
\[
\mathbf{Y} = f_\theta (\mathbf{X}) 
\]
and MLP can be used to find a model. The problem with the MLP model is
that the length of the sequences $\mathbf{X}$ and $\mathbf{Y}$ can vary,
but MLP must have a fixed number of inputs and outputs. For iterative
forecasting, the number of outputs can be set to one, and the number
of inputs can be 'big enough', and if the input sequence is
shorter than the maximum limit, \textit{padding inputs} are added.
However, such a network grows rapidly for an increasing number of
inputs and soon becomes useless for practical applications where only
a limited number of training samples are available.

The number of inputs can be reduced, only one past example being the
limit. In this case, the MLP learns the mapping
\[
x_{n+1} = f_\theta (x_{n}) \enspace ,
\]
but this naïve approach does not work very well. For example, it
cannot model sinusoidal signals since each value except 0 and 1
appears in two ’contexts'; the raisin and falling edges of the wave.
t the same time, it is well known that some problems need to address
both short and long-horizon dependencies to predict the next value.
This problem can be solved by Recurrent Neural Networks (RNNs), in
theory, and by Transformers, in practice.


\paragraph{Sequence-to-sequence (Seq2Seq) modeling.}
In the deep learning literature, Seq2Seq models are discussed with
Recurrent Neural Networks (RNNs). The main difference between the MLP
and RNN models is that RNNs maintain a system 'state' $h_t$ that is
always passed forward to the next prediction. The RNN model can be
defined as
\[
x_{n+1},h_{n+1} = f_\theta (x_{n},h_{n+1}) \enspace .
\]

Theoretically, RNNs can have infinitely long 'memory', but in practice,
that would mean that gradient information would be maintained over a
long period of time, which is difficult with limited precision of
computer arithmetics and time. 

Many RNN architectures have been proposed, and one of the most popular
is Long Short-Term Memory (LSTM)~\cite{LSTM}.


\paragraph{Transformer architecture.}
To understand the design choices of the Transformer architecture, it
can be beneficial to learn about the progression of RNNs in machine
translation where one language is mapped to another
(English-to-French). This Seq2Seq problem is more general than the
time series forecasting since the type of input and output are
different (English vs. French vocabulary).


A very good work that describes the principled idea of using RNNs
(LSTM) inmachine translation is~\cite{Sutskever-2014-neurips}.
The encoder LSTM encodes the input sequence of $N$ into the state
variable. In practice, that is the last state $h_n$ of the LSTM. Then
the state vector is passed to the decoder LSTM that starts to generate
output. The starts and ends of both sequences are encoded using
special symbols Start-of-Sequence (SOS) and End-of-Sequence (EOS).

A similar structure was used in~\cite{Cho-2014-emnlp}, which is a good
read after~\cite{Sutskever-2014-neurips}. They note that such a
structure quickly 'forgets' history and propose a mechanism that uses
information from all past samples to infer the next prediction(s).
This mechanism is \textit{attention}. In their model,  all past state
vectors are stored, multiplied by \textit{attention weights}, and
summed for the decoder. Similarly, the decoder can use its previous
outputs via \textit{self-attention}. This approach can use all input samples in
inference, but its main bottleneck is the sequential process of RNNs
which makes training and inference slow. This attention mechanism is
often referred to as \textit{Bahdanau attention}.

Finally, after the two previous and influential works, the seminal
Transformer paper ``Attention is All You Need'' is much easier to
comprehend with all its flavors~\cite{transformer}. The most important
idea is to avoid the sequential processing of RNNs. This is achieved
by multiplicative attention that exploits the neural computation
library (Torch, Keras) functions ability to automatically forward any
number of inputs and backward the gradients from the loss function. In
many ways, Transformer is more like clever software engineering than
new AI theory.


\section{Methods}


\begin{ack}
This work would not be possible without tenure in Tampere
University. My tenured position allows me to allocate time to projects
that benefit society at large, or benefit nobody if they ideas just
fail. I hope that you cite this work if you find it useful since that
helps me to justify the time and efforts used for this writing this
article and preparing the code examples.

Universities are beautiful
places and research, teaching, and supervising is the best I know, but
I am afraid that blind believe in academic metrics and the corporate
style management of universities will destroy these holy places of
academic work.
\end{ack}

\printbibliography

\end{document}
