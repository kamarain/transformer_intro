{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69aa7a6d-c26d-4f74-bbb9-2d10a193a6aa",
   "metadata": {},
   "source": [
    "# Introduction to Sequence Modeling with Transformers\n",
    "\n",
    "This Jupyter notebook accompanies the following article (please, cite in your work or Web page if you find it useful):\n",
    "\n",
    " * TO BE DEFINED\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4539a705-119c-4e71-8946-44fba9329409",
   "metadata": {},
   "source": [
    "## Torch.nn.Transformer\n",
    "\n",
    "It seems that the generic Transformer module of PyTorch is becoming depricated, but since it conveniently hides all details about implementing the transformer architecture itself, it is used in these code examples.\n",
    "\n",
    "**Note:** This code does not necessarily work with other implementations of Transformer\n",
    "\n",
    "Fundamental imports are the following and this code is tested on Python 3.12.2 and everything installed through Linux Conda package manager."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 605,
   "id": "cb6886dc-d8dd-40b6-9570-55f6d10016b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "\n",
    "import math\n",
    "import numpy as np\n",
    "import numpy.random as random\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import platform\n",
    "\n",
    "print(platform.python_version())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70b2353f-7deb-4136-9c19-fb9928a5e120",
   "metadata": {},
   "source": [
    "## PlainTransformer - Transformer and nothing but the transformer\n",
    "\n",
    "Define a PlainTransformer class that has [nn.torch.Transformer](https://pytorch.org/docs/stable/generated/torch.nn.Transformer.html) as a member variable.\n",
    "\n",
    "The class can take take a sequence (e.g. [0, 0, 0, 0]) as input, then puts it through the torch.nn.Transformer model, and gives the Transformer as output.\n",
    "\n",
    "**Note:** The only additional change to the default parameters is `norm_first = True` since the original plain transformer architecture had difficulty converging, but it was later found that doing normalization *before* multiheaded attention stabilizes learning (no heatup with increasing learning rate needed in the beginning of training)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 606,
   "id": "0c81cef0-bf48-4398-bd5a-8b0b58fee38f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PlainTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        d_model,\n",
    "        nhead,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dim_feedforward,\n",
    "        dropout_p,\n",
    "        layer_norm_eps\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # Transformer initialized with user specs\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model = d_model,\n",
    "            nhead = nhead,\n",
    "            num_encoder_layers = num_encoder_layers,\n",
    "            num_decoder_layers = num_decoder_layers,\n",
    "            dim_feedforward = dim_feedforward,\n",
    "            dropout = dropout_p,\n",
    "            layer_norm_eps = layer_norm_eps,\n",
    "            norm_first = True\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src,\n",
    "        tgt,\n",
    "    ):\n",
    "        # Transformer assumes that src & tgt structure is (seq_length, batch_num, feat_dim)\n",
    "        out = self.transformer(src, tgt)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76a77195-7686-4afe-86d0-d0939f274c6f",
   "metadata": {},
   "source": [
    "Let's define PlainTransformer with sminimal settings. Everything is one, but to make the linear feedforward part to have any meaning, we assign 8 neurons there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 607,
   "id": "f2403825-124d-4ca9-abbd-c05bfb0d88a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 88 parameters (88 trainable)\n"
     ]
    }
   ],
   "source": [
    "model = PlainTransformer(d_model = 1, nhead = 1, num_encoder_layers = 1, num_decoder_layers = 1, dim_feedforward = 8, dropout_p = 0.1, layer_norm_eps = 1e-05)\n",
    "\n",
    "#print(model)\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters())} parameters ({sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable)')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "219d9f6d-8be9-4db9-a620-9a1941593ab9",
   "metadata": {},
   "source": [
    "**Training data**\n",
    "\n",
    "Let's start with two symbols only:\n",
    "\n",
    " * 0\n",
    " * 1\n",
    "\n",
    "The two symbols can be used in binary sequences. The simplest task is perhaps to continue the sequences. The training data is\n",
    "\n",
    " * X: 0, 0, 0 $\\rightarrow$ Y: 0, 0, 0\n",
    " * X: 1, 1, 1 $\\rightarrow$ Y: 1, 1, 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 608,
   "id": "1a28adb4-aea7-47da-adc1-af24b648f695",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data1(n):\n",
    "    SOS_token = np.array([2])\n",
    "    EOS_token = np.array([3])\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # 1,1,1,1 -> 1,1,1,1\n",
    "    for i in range(n // 2):\n",
    "        X = np.ones(4)\n",
    "        y = np.ones(4)\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 0,0,0,0 -> 0,0,0,0\n",
    "    for i in range(n // 2):\n",
    "        X = np.zeros(4)\n",
    "        y = np.zeros(4)\n",
    "        data.append([X, y])\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53e7a11d-af1e-467a-a0d5-ff551667bb83",
   "metadata": {},
   "source": [
    "Run several times to make sure both types of sequences are generated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 609,
   "id": "65e74ca4-1d13-4297-8690-954df893543b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n",
      "[1. 1. 1. 1.]\n",
      "[1. 1. 1. 1.]\n",
      "[0. 0. 0. 0.]\n",
      "[0. 0. 0. 0.]\n"
     ]
    }
   ],
   "source": [
    "tr_data = generate_data1(100)\n",
    "\n",
    "print(tr_data[0][0])\n",
    "print(tr_data[0][1])\n",
    "\n",
    "print(tr_data[1][0])\n",
    "print(tr_data[1][1])\n",
    "\n",
    "print(tr_data[2][0])\n",
    "print(tr_data[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc2f1c1-9afb-4b26-a4fa-dc45ae563863",
   "metadata": {},
   "source": [
    "Form training matrices of ```Seq len x Num samples x Feat dim```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 610,
   "id": "d3f049f7-14e1-4ad8-a9a6-0da5cba7e6f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 100, 1])\n",
      "tensor([0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([1., 1., 1., 1.])\n",
      "tensor([0., 0., 0., 0.])\n",
      "tensor([0., 0., 0., 0.])\n"
     ]
    }
   ],
   "source": [
    "X_tr = torch.empty((len(tr_data[0][0]),len(tr_data),1))\n",
    "Y_tr = torch.empty((len(tr_data[0][1]),len(tr_data),1))\n",
    "for ids, s in enumerate(tr_data):\n",
    "    X_tr[:,ids,0] = torch.from_numpy(s[0])\n",
    "    Y_tr[:,ids,0] = torch.from_numpy(s[1])\n",
    "print(X_tr.shape)\n",
    "\n",
    "print(X_tr[:,0,0])\n",
    "print(Y_tr[:,0,0])\n",
    "\n",
    "print(X_tr[:,1,0])\n",
    "print(Y_tr[:,1,0])\n",
    "\n",
    "print(X_tr[:,2,0])\n",
    "print(Y_tr[:,2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 611,
   "id": "2fb250fa-8678-49e6-8292-936ceb1e94d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 0 training loss 0.5 (lr=0.01)\n",
      "   Epoch 100 training loss 0.2500151991844177 (lr=0.01)\n",
      "   Epoch 200 training loss 0.25 (lr=0.01)\n",
      "Final:   Epoch 299 training loss 0.25 (lr=0.01)\n"
     ]
    }
   ],
   "source": [
    "num_of_epochs = 300\n",
    "loss_mse = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[1000], gamma=0.1)\n",
    "model.train()\n",
    "for n in range(num_of_epochs):\n",
    "    running_loss = 0.0\n",
    "    Y_in = Y_tr[:-1,:,:]\n",
    "    Y_out = Y_tr[1:,:,:]\n",
    "    Y_pred = model(X_tr,Y_in)\n",
    "    loss = loss_mse(Y_pred,Y_out)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    if n % 100 == 0:\n",
    "        print(f'   Epoch {n} training loss {running_loss} (lr={optimizer.param_groups[0][\"lr\"]})')\n",
    "print(f'Final:   Epoch {n} training loss {running_loss} (lr={optimizer.param_groups[0][\"lr\"]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 612,
   "id": "3f29a717-28fd-4d43-bbb5-0843849fb628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0\n",
      "Input: [0, 0, 0, 0]\n",
      "Target: [0, 0, 0, 0]\n",
      "Output: [0.5000001192092896, 0.5000001192092896, 0.5000001192092896, 0.5000001192092896]\n",
      "\n",
      "Example 1\n",
      "Input: [1, 1, 1, 1]\n",
      "Target: [1, 1, 1, 1]\n",
      "Output: [0.5000001192092896, 0.5000001192092896, 0.5000001192092896, 0.5000001192092896]\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_4931/2730787215.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  src = torch.tensor(torch.empty((4,1,1)))\n"
     ]
    }
   ],
   "source": [
    "# Here we test some examples to observe how the model predicts\n",
    "examples = [\n",
    "    torch.tensor([0, 0, 0, 0], dtype=torch.long),\n",
    "    torch.tensor([1, 1, 1, 1], dtype=torch.long)\n",
    "]\n",
    "\n",
    "model.eval()\n",
    "\n",
    "for idx, example in enumerate(examples):\n",
    "    src = torch.tensor(torch.empty((4,1,1)))\n",
    "    src[:,0,0] = examples[idx]\n",
    "    result = model(src, src)\n",
    "    print(f\"Example {idx}\")\n",
    "    print(f\"Input: {example.view(-1).tolist()}\")\n",
    "    print(f\"Target: {example.view(-1).tolist()}\")\n",
    "    print(f\"Output: {torch.squeeze(result).tolist()}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23cc85cb-a1aa-4a4f-85d4-6b6d4c98a527",
   "metadata": {},
   "source": [
    "**Findings:** Transformer is suitable for providing information about the most likely next token given the input sequence $\\mathbf{X}$ and the current output sequence $\\mathbf{Y}$. However, it the Transformer output is used as the next output sequence token, then it cannot do better than to provide a single token that minimizes the mean squared error (MSE). In the case of equal amount of '1' and '0' as output it learns to predict 0.5 that is between the two values. You can change the values, e.g., to '0' and '10' to validate this finding.\n",
    "\n",
    "To improve PlainTransformer we must introduce a mapping from 'tokens' (sequence symbols) to an internal representation that carries out information about the best next token, and then again back to tokens. These two steps are called *embedding* and *unembedding*. They convert every token to a token specific vector (of the length `d_model`) and back. Embedding is a table lookup from 1 to model dimensional vector, but the vector representation can be optimized during training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bd747f9-495c-4eb8-a045-e087f36a8d59",
   "metadata": {},
   "source": [
    "## TokenTransformer - Add token embedding and un-embedding\n",
    "\n",
    "At first, let's turn from integers to talk about *tokens*. The input and output sequences, $\\textbf{X}$ and $\\textbf{Y}$, are sequences of tokens with unique (integer) Id. Let's use the following tokens next:\n",
    "\n",
    "```\n",
    "Symbol 0 is 0\n",
    "Symbol 1 is 1\n",
    "Symbol <SOS> is 2 // Start of sequence\n",
    "Symbol <EOS> is 3 // End of sequence\n",
    "```\n",
    "\n",
    "The SOS and EOS tokens help the transformer to learn the beginning and end of sequences."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86160ac4-3f86-4d7c-81cb-ca3bc7650149",
   "metadata": {},
   "source": [
    "Generate tokenized data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f1fa239-b60d-4d1e-9d85-f2c9f2a6725d",
   "metadata": {},
   "source": [
    "We have the following tokens:\n",
    "\n",
    " *       `0` : 0 \n",
    " *       `1` : 1 \n",
    " * `<SOS>` : 2\n",
    " * `<EOS>` : 3\n",
    "\n",
    "Let's try to generate a single output from a sequence input\n",
    "\n",
    " * input: SOS,0,0,0,EOS  output: SOS,0,EOS\n",
    " * input: SOS,1,1,1,EOS  output: SOS,1,EOS\n",
    "\n",
    "or vice versa\n",
    "\n",
    " * SOS,0,EOS -> SOS,0,0,0,EOS\n",
    " * SOS,1,EOS -> SOS,1,1,1,EOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 613,
   "id": "8b0de8fe-b1b2-436e-bb93-d86f0d6117fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data1_tokens(n):\n",
    "    SOS_token = np.array([2])\n",
    "    EOS_token = np.array([3])\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # 0,0,0,0 -> 0,0,0,0\n",
    "    for i in range(n // 2):\n",
    "        X = np.concatenate((SOS_token, [0, 0, 0, 0], EOS_token))\n",
    "        y = np.concatenate((SOS_token, [0, 0, 0, 0], EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 1,1,1,1 -> 1,1,1,1 \n",
    "    for i in range(n // 2):\n",
    "        X = np.concatenate((SOS_token, [1, 1, 1, 1], EOS_token))\n",
    "        y = np.concatenate((SOS_token, [1, 1, 1, 1], EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f48d91e2-58a7-460c-aa92-f14c51c9abb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data2(n):\n",
    "    SOS_token = np.array([2])\n",
    "    EOS_token = np.array([3])\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # 0,0,0 -> 1 \n",
    "    for i in range(n // 2):\n",
    "        X = np.concatenate((SOS_token, [0, 0, 0], EOS_token))\n",
    "        y = np.concatenate((SOS_token, [1], EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 1,1,1 -> 0 \n",
    "    for i in range(n // 2):\n",
    "        X = np.concatenate((SOS_token, [1, 1, 1], EOS_token))\n",
    "        y = np.concatenate((SOS_token, [0], EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "126b6ed9-e129-4d7e-a6fd-4ca09cea54f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data3(n):\n",
    "    SOS_token = np.array([2])\n",
    "    EOS_token = np.array([3])\n",
    "\n",
    "    data = []\n",
    "        \n",
    "    # 1 -> 0,0,0 \n",
    "    for i in range(n // 2):\n",
    "        y = np.concatenate((SOS_token, [0, 0, 0], EOS_token))\n",
    "        X = np.concatenate((SOS_token, [1], EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 1 -> 1,1,1 \n",
    "    for i in range(n // 2):\n",
    "        y = np.concatenate((SOS_token, [1, 1, 1], EOS_token))\n",
    "        X = np.concatenate((SOS_token, [0], EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 621,
   "id": "1bfa817a-8a2c-46f4-a564-193875af1d5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 1 1 3]\n",
      "[2 0 3]\n",
      "[2 0 0 0 3]\n",
      "[2 1 3]\n",
      "[2 0 0 0 3]\n",
      "[2 1 3]\n"
     ]
    }
   ],
   "source": [
    "#tr_data = generate_data1_tokens(100)\n",
    "tr_data = generate_data2(100)\n",
    "#tr_data = generate_data3(100)\n",
    "\n",
    "\n",
    "print(tr_data[0][0])\n",
    "print(tr_data[0][1])\n",
    "\n",
    "print(tr_data[1][0])\n",
    "print(tr_data[1][1])\n",
    "\n",
    "print(tr_data[2][0])\n",
    "print(tr_data[2][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dfa3dc5-aeaa-498f-bf8a-c1a20b73b073",
   "metadata": {},
   "source": [
    "Generate training data tensors. Note that now we do not generate extra dimension (seq len x num samples x 1) for the tokens that are one-dimensional since embedding will add the requested number of dimensions (seq len x num samples x num feats)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 622,
   "id": "a8f91756-2dd4-4dbb-850f-fa4f95e78ffb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([5, 100])\n",
      "tensor([2., 1., 1., 1., 3.])\n",
      "tensor([2., 0., 3.])\n",
      "tensor([2., 0., 0., 0., 3.])\n",
      "tensor([2., 1., 3.])\n",
      "tensor([2., 0., 0., 0., 3.])\n",
      "tensor([2., 1., 3.])\n"
     ]
    }
   ],
   "source": [
    "X_tr = torch.empty((len(tr_data[0][0]),len(tr_data)))\n",
    "Y_tr = torch.empty((len(tr_data[0][1]),len(tr_data)))\n",
    "for ids, s in enumerate(tr_data):\n",
    "    X_tr[:,ids] = torch.from_numpy(s[0])\n",
    "    Y_tr[:,ids] = torch.from_numpy(s[1])\n",
    "print(X_tr.shape)\n",
    "\n",
    "print(X_tr[:,0])\n",
    "print(Y_tr[:,0])\n",
    "\n",
    "print(X_tr[:,1])\n",
    "print(Y_tr[:,1])\n",
    "\n",
    "print(X_tr[:,2])\n",
    "print(Y_tr[:,2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce352c8-a925-442d-86b7-30b34994853c",
   "metadata": {},
   "source": [
    "Tokenization does not help as such, but for each token a special *embedding vector* can be generated. Embedding vector may carry more information than a single value.\n",
    "\n",
    "**Note:** Unembedding is actually performed by a linear layer that converts the embedding vector to a vector with the same length as the number of tokens. Now, the next token selection becomes a classification task. We assume that the unembedding vector contains probabilities of each token. By using the *cross-entropy error* as loss function we can optimize this mapping. Note that [torch.nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html) performs the soft-max operation internally and therefore it does not need to be part of the Transformer architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 623,
   "id": "10bede6b-d8d9-4549-a437-731cba3e17fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TokenTransformer(nn.Module):\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens,\n",
    "        d_model,\n",
    "        nhead,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dim_feedforward,\n",
    "        dropout_p,\n",
    "        layer_norm_eps\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Token embedding layer - this takes care of converting integer ids to vectors\n",
    "        self.embedding = nn.Embedding(num_tokens, d_model)\n",
    "\n",
    "        # Token \"unembedding\" to one-hot encoded token vector\n",
    "        self.unembedding = nn.Linear(d_model, num_tokens)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model = d_model,\n",
    "            nhead = nhead,\n",
    "            num_encoder_layers = num_encoder_layers,\n",
    "            num_decoder_layers = num_decoder_layers,\n",
    "            dim_feedforward = dim_feedforward,\n",
    "            dropout = dropout_p,\n",
    "            layer_norm_eps = layer_norm_eps,\n",
    "            norm_first = True\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src,\n",
    "        tgt,\n",
    "    ):\n",
    "        # Note: src & tgt default size is (seq_length, batch_num, feat_dim)\n",
    "\n",
    "        # Token embedding\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)        \n",
    "\n",
    "        # Transformer output\n",
    "        out = self.transformer(src, tgt)\n",
    "        out = self.unembedding(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 624,
   "id": "6fde67d1-63e4-4aab-bf84-a423167c0017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1332 parameters (1332 trainable)\n"
     ]
    }
   ],
   "source": [
    "model = TokenTransformer(num_tokens = 4, d_model = 8, nhead = 1, num_encoder_layers = 1, num_decoder_layers = 1, dim_feedforward = 8, dropout_p = 0.1, layer_norm_eps = 1e-05)\n",
    "\n",
    "#print(model)\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters())} parameters ({sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 618,
   "id": "b8a9f198-fc4a-4f6d-9a9a-50aeb88b0b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 0 training loss 1.709815502166748 (lr=0.01)\n",
      "   Epoch 10 training loss 0.7629984021186829 (lr=0.01)\n",
      "   Epoch 20 training loss 0.6277658939361572 (lr=0.01)\n",
      "   Epoch 30 training loss 0.5643086433410645 (lr=0.01)\n",
      "   Epoch 40 training loss 0.5294503569602966 (lr=0.01)\n",
      "   Epoch 50 training loss 0.502356767654419 (lr=0.01)\n",
      "   Epoch 60 training loss 0.4851935803890228 (lr=0.01)\n",
      "   Epoch 70 training loss 0.4668138325214386 (lr=0.01)\n",
      "   Epoch 80 training loss 0.4707247018814087 (lr=0.01)\n",
      "   Epoch 90 training loss 0.46889474987983704 (lr=0.01)\n",
      "   Epoch 100 training loss 0.46010443568229675 (lr=0.001)\n",
      "   Epoch 110 training loss 0.46775081753730774 (lr=0.001)\n",
      "   Epoch 120 training loss 0.47330453991889954 (lr=0.001)\n",
      "   Epoch 130 training loss 0.4585135579109192 (lr=0.001)\n",
      "   Epoch 140 training loss 0.4561307728290558 (lr=0.001)\n",
      "   Epoch 150 training loss 0.4628158509731293 (lr=0.001)\n",
      "   Epoch 160 training loss 0.46612247824668884 (lr=0.001)\n",
      "   Epoch 170 training loss 0.4642239809036255 (lr=0.001)\n",
      "   Epoch 180 training loss 0.4674522280693054 (lr=0.001)\n",
      "   Epoch 190 training loss 0.46958988904953003 (lr=0.001)\n",
      "   Epoch 200 training loss 0.46251967549324036 (lr=0.001)\n",
      "   Epoch 210 training loss 0.4615556299686432 (lr=0.001)\n",
      "   Epoch 220 training loss 0.4661429226398468 (lr=0.001)\n",
      "   Epoch 230 training loss 0.46592196822166443 (lr=0.001)\n",
      "   Epoch 240 training loss 0.4581749141216278 (lr=0.001)\n",
      "   Epoch 250 training loss 0.4598652124404907 (lr=0.001)\n",
      "   Epoch 260 training loss 0.4600098729133606 (lr=0.001)\n",
      "   Epoch 270 training loss 0.45304974913597107 (lr=0.001)\n",
      "   Epoch 280 training loss 0.45905131101608276 (lr=0.001)\n",
      "   Epoch 290 training loss 0.46259650588035583 (lr=0.001)\n",
      "   Epoch 300 training loss 0.46537545323371887 (lr=0.001)\n",
      "   Epoch 310 training loss 0.46408969163894653 (lr=0.001)\n",
      "   Epoch 320 training loss 0.4641277492046356 (lr=0.001)\n",
      "   Epoch 330 training loss 0.4596921503543854 (lr=0.001)\n",
      "   Epoch 340 training loss 0.4592130184173584 (lr=0.001)\n",
      "   Epoch 350 training loss 0.4604368805885315 (lr=0.001)\n",
      "   Epoch 360 training loss 0.4649761915206909 (lr=0.001)\n",
      "   Epoch 370 training loss 0.46437597274780273 (lr=0.001)\n",
      "   Epoch 380 training loss 0.4577884078025818 (lr=0.001)\n",
      "   Epoch 390 training loss 0.46103909611701965 (lr=0.001)\n",
      "   Epoch 400 training loss 0.4688098132610321 (lr=0.001)\n",
      "   Epoch 410 training loss 0.45646339654922485 (lr=0.001)\n",
      "   Epoch 420 training loss 0.45184755325317383 (lr=0.001)\n",
      "   Epoch 430 training loss 0.46028584241867065 (lr=0.001)\n",
      "   Epoch 440 training loss 0.4565870463848114 (lr=0.001)\n",
      "   Epoch 450 training loss 0.45934829115867615 (lr=0.001)\n",
      "   Epoch 460 training loss 0.4525720775127411 (lr=0.001)\n",
      "   Epoch 470 training loss 0.46227824687957764 (lr=0.001)\n",
      "   Epoch 480 training loss 0.4576185345649719 (lr=0.001)\n",
      "   Epoch 490 training loss 0.4545614719390869 (lr=0.001)\n",
      "   Epoch 500 training loss 0.45925918221473694 (lr=0.001)\n",
      "   Epoch 510 training loss 0.4625724256038666 (lr=0.001)\n",
      "   Epoch 520 training loss 0.45839667320251465 (lr=0.001)\n",
      "   Epoch 530 training loss 0.4557368755340576 (lr=0.001)\n",
      "   Epoch 540 training loss 0.4571355879306793 (lr=0.001)\n",
      "   Epoch 550 training loss 0.4611760675907135 (lr=0.001)\n",
      "   Epoch 560 training loss 0.457457035779953 (lr=0.001)\n",
      "   Epoch 570 training loss 0.45248231291770935 (lr=0.001)\n",
      "   Epoch 580 training loss 0.4564878046512604 (lr=0.001)\n",
      "   Epoch 590 training loss 0.46248650550842285 (lr=0.001)\n",
      "   Epoch 600 training loss 0.45519015192985535 (lr=0.001)\n",
      "   Epoch 610 training loss 0.4552769660949707 (lr=0.001)\n",
      "   Epoch 620 training loss 0.45257115364074707 (lr=0.001)\n",
      "   Epoch 630 training loss 0.4552222490310669 (lr=0.001)\n",
      "   Epoch 640 training loss 0.4548446536064148 (lr=0.001)\n",
      "   Epoch 650 training loss 0.4555511176586151 (lr=0.001)\n",
      "   Epoch 660 training loss 0.4548139274120331 (lr=0.001)\n",
      "   Epoch 670 training loss 0.45141300559043884 (lr=0.001)\n",
      "   Epoch 680 training loss 0.4610201418399811 (lr=0.001)\n",
      "   Epoch 690 training loss 0.4527755677700043 (lr=0.001)\n",
      "   Epoch 700 training loss 0.45553988218307495 (lr=0.001)\n",
      "   Epoch 710 training loss 0.4580051600933075 (lr=0.001)\n",
      "   Epoch 720 training loss 0.4537607729434967 (lr=0.001)\n",
      "   Epoch 730 training loss 0.4519566297531128 (lr=0.001)\n",
      "   Epoch 740 training loss 0.4569922089576721 (lr=0.001)\n",
      "   Epoch 750 training loss 0.4547475576400757 (lr=0.001)\n",
      "   Epoch 760 training loss 0.457855224609375 (lr=0.001)\n",
      "   Epoch 770 training loss 0.4585362374782562 (lr=0.001)\n",
      "   Epoch 780 training loss 0.44916069507598877 (lr=0.001)\n",
      "   Epoch 790 training loss 0.4524257481098175 (lr=0.001)\n",
      "   Epoch 800 training loss 0.45380935072898865 (lr=0.001)\n",
      "   Epoch 810 training loss 0.45357581973075867 (lr=0.001)\n",
      "   Epoch 820 training loss 0.45314300060272217 (lr=0.001)\n",
      "   Epoch 830 training loss 0.452992707490921 (lr=0.001)\n",
      "   Epoch 840 training loss 0.453232079744339 (lr=0.001)\n",
      "   Epoch 850 training loss 0.45278942584991455 (lr=0.001)\n",
      "   Epoch 860 training loss 0.45548710227012634 (lr=0.001)\n",
      "   Epoch 870 training loss 0.4534710943698883 (lr=0.001)\n",
      "   Epoch 880 training loss 0.4526926875114441 (lr=0.001)\n",
      "   Epoch 890 training loss 0.4588514566421509 (lr=0.001)\n",
      "   Epoch 900 training loss 0.4553494155406952 (lr=0.001)\n",
      "   Epoch 910 training loss 0.4556256830692291 (lr=0.001)\n",
      "   Epoch 920 training loss 0.4516083598136902 (lr=0.001)\n",
      "   Epoch 930 training loss 0.4571148455142975 (lr=0.001)\n",
      "   Epoch 940 training loss 0.45675426721572876 (lr=0.001)\n",
      "   Epoch 950 training loss 0.45537781715393066 (lr=0.001)\n",
      "   Epoch 960 training loss 0.4516099691390991 (lr=0.001)\n",
      "   Epoch 970 training loss 0.4553144574165344 (lr=0.001)\n",
      "   Epoch 980 training loss 0.45329487323760986 (lr=0.001)\n",
      "   Epoch 990 training loss 0.4490432143211365 (lr=0.001)\n",
      "Final:   Epoch 999 training loss 0.45029255747795105 (lr=0.001)\n"
     ]
    }
   ],
   "source": [
    "num_of_epochs = 1000\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[100], gamma=0.1)\n",
    "model.train()\n",
    "for n in range(num_of_epochs):\n",
    "    running_loss = 0.0\n",
    "    X_in = X_tr.long()\n",
    "    Y_in = Y_tr[:-1,:].long()\n",
    "    Y_out = Y_tr[1:,:].long()\n",
    "    Y_pred = model(X_in,Y_in)\n",
    "    # seq len x num samples => num samples x seq len\n",
    "    Y_out = Y_out.permute(1,0)\n",
    "    # seq len x num samples x token one hot => num samples x token one hot x seq len\n",
    "    Y_pred = Y_pred.permute(1, 2, 0)\n",
    "    loss = loss_fn(Y_pred,Y_out)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    if n % 10 == 0:\n",
    "        print(f'   Epoch {n} training loss {running_loss} (lr={optimizer.param_groups[0][\"lr\"]})')\n",
    "print(f'Final:   Epoch {n} training loss {running_loss} (lr={optimizer.param_groups[0][\"lr\"]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62eb1d68-0eb3-49a5-9fb9-a516cc19b91e",
   "metadata": {},
   "source": [
    "Function for predicting values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 619,
   "id": "26835699-7457-4dd3-ac12-cfecafb33d1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_sequence, max_length=16, SOS_token=2, EOS_token=3, EOS_plus = 2):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    y_input = torch.tensor([SOS_token], dtype=torch.long)\n",
    "\n",
    "    sos_found = False\n",
    "    for _ in range(max_length):\n",
    "        #print(f'X={input_sequence} Y={y_input}')\n",
    "\n",
    "    \n",
    "        pred = model(input_sequence, y_input)\n",
    "\n",
    "        pred_tokens = torch.argmax(pred, dim=1)\n",
    "\n",
    "        y_input = torch.cat((torch.tensor([SOS_token], dtype=torch.long), pred_tokens), dim=0)\n",
    "\n",
    "        if pred_tokens[-1] == EOS_token:\n",
    "            break\n",
    "\n",
    "    return y_input.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 620,
   "id": "da8dbfc1-96a2-43bf-a4cc-48bd07eb4ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0\n",
      "Input sequence: [1, 1, 1, 1]\n",
      "Output (predicted) sequence: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Example 1\n",
      "Input sequence: [0, 0, 0, 0]\n",
      "Output (predicted) sequence: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Example 2\n",
      "Input sequence: [1, 1, 1]\n",
      "Output (predicted) sequence: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Example 3\n",
      "Input sequence: [0, 0, 0]\n",
      "Output (predicted) sequence: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "Example 4\n",
      "Input sequence: [1]\n",
      "Output (predicted) sequence: [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "\n",
      "Example 5\n",
      "Input sequence: [0]\n",
      "Output (predicted) sequence: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we test some examples to observe how the model predicts\n",
    "examples = [\n",
    "    torch.tensor([2,1, 1, 1, 1,3], dtype=torch.long),\n",
    "    torch.tensor([2,0, 0, 0, 0,3], dtype=torch.long),\n",
    "    torch.tensor([2, 1, 1, 1, 3], dtype=torch.long),\n",
    "    torch.tensor([2, 0, 0, 0, 3], dtype=torch.long),\n",
    "    torch.tensor([2, 1, 3], dtype=torch.long),\n",
    "    torch.tensor([2, 0, 3], dtype=torch.long)\n",
    "]\n",
    "\n",
    "for idx, example in enumerate(examples):\n",
    "    result = predict(model, example)\n",
    "    print(f\"Example {idx}\")\n",
    "    print(f\"Input sequence: {example.view(-1).tolist()[1:-1]}\")\n",
    "    print(f\"Output (predicted) sequence: {result[1:-1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b99557f0-1efe-4932-a12f-18984f30ba71",
   "metadata": {},
   "source": [
    "**Findings**\n",
    "\n",
    "Interestingly, the ```SimpleTransformer``` architecture learns to map long sequences to short, e.g., 0,0,0,0 -> 1 and 1,1,1,1 -> 0 , but not short sequences to long, e.g., 0 -> 1,1,1,1 and 1 -> 0,0,0,0.\n",
    "\n",
    "The problem is that we train it incorrectly.\n",
    "\n",
    "SimpleTransformer is trained with full sequences\n",
    "\n",
    " * SOS,1,EOS -> SOS,0,0,0,0,EOS\n",
    " * SOS,0,EOS -> SOS,1,1,1,1,EOS\n",
    "\n",
    " BUT during inference it only knows the input and what is generated so far\n",
    "\n",
    " * (SOS,1,EOS),(SOS) -> 0\n",
    " * (SOS,1,EOS),(SOS,0) -> 0,0\n",
    " * (SOS,1,EOS),(SOS,0,0) -> 0,0,0\n",
    " * (SOS,1,EOS),(SOS,0,0,0) -> 0,0,0,0\n",
    " * (SOS,1,EOS),(SOS,0,0,0,0) -> 0,0,0,0,EOS\n",
    "\n",
    "This means that inference is different from training, and especially in the short-to-long case.\n",
    "\n",
    "This can be fixed by **masking** the future outputs during taining. After this little extension the transformer is already pretty powerful model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6eb7761d-f0df-4d33-8dd4-86a10043aa2f",
   "metadata": {},
   "source": [
    "## MaskedTokenTransformer - Masking the future outputs during training and inference\n",
    "\n",
    "```SimpleTransformer```was able to learn to produce a single output from multiple, but not multiple outputs from single. The main reason for that is that there is discrepancy between the training data and test data. During training it was aware of the full future, i.e., its task was to learn to produce a missing token in the following cases\n",
    "\n",
    " * SOS,1,EOS -> SOS,?,0,0,,EOS\n",
    " * SOS,1,EOS -> SOS,0,?,0,EOS\n",
    " * SOS,1,EOS -> SOS,0,0,?,EOS\n",
    " * SOS,1,EOS -> SOS,0,0,0,?\n",
    "\n",
    "while during testin the 'future' outputs are not known and the problem is actually\n",
    "\n",
    "\n",
    " * SOS,1,EOS -> SOS,?\n",
    " * SOS,1,EOS -> SOS,0,?\n",
    " * SOS,1,EOS -> SOS,0,0,?\n",
    " * SOS,1,EOS -> SOS,0,0,0,?\n",
    "\n",
    "which is different from the training. Note that in the many-to-one case training and test cases are almost identical and that is why it worked.\n",
    "\n",
    "**Solution:** During training we need to mask the future outputs. There is a pre-made Torch function for that"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "423ce223-4f6b-47a5-9421-ada9845ef096",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., -inf, -inf, -inf, -inf],\n",
      "        [0., 0., -inf, -inf, -inf],\n",
      "        [0., 0., 0., -inf, -inf],\n",
      "        [0., 0., 0., 0., -inf],\n",
      "        [0., 0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "sequence_length = 5\n",
    "tgt_mask = nn.Transformer.generate_square_subsequent_mask(sequence_length)\n",
    "print(tgt_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20cd6a41-7c34-4d51-9154-263939ebdd88",
   "metadata": {},
   "source": [
    "Let's define ```SimpleTransformerPlus``` that supports masking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bd2b5720-17f3-4df4-9991-337049b735fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaskedTokenTransformer(nn.Module):\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens,\n",
    "        d_model,\n",
    "        nhead,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dim_feedforward,\n",
    "        dropout_p,\n",
    "        layer_norm_eps\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Token embedding layer - this takes care of converting integer to vectors\n",
    "        self.embedding = nn.Embedding(num_tokens, d_model)\n",
    "\n",
    "        # Token \"unembedding\" to one-hot token vector\n",
    "        self.unembedding = nn.Linear(d_model, num_tokens)\n",
    "\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model = d_model,\n",
    "            nhead = nhead,\n",
    "            num_encoder_layers = num_encoder_layers,\n",
    "            num_decoder_layers = num_decoder_layers,\n",
    "            dim_feedforward = dim_feedforward,\n",
    "            dropout = dropout_p,\n",
    "            layer_norm_eps = layer_norm_eps,\n",
    "            norm_first = True\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src,\n",
    "        tgt,\n",
    "        tgt_mask = None\n",
    "    ):\n",
    "        # Note: src & tgt default size is (seq_length, batch_num, feat_dim)\n",
    "\n",
    "        # Token embedding\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)        \n",
    "\n",
    "        # Transformer output\n",
    "        out = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        out = self.unembedding(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3a7985bf-f5a8-41cf-9054-fef82a8ac66b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1332 parameters (1332 trainable)\n"
     ]
    }
   ],
   "source": [
    "model = MaskedTokenTransformer(num_tokens = 4, d_model = 8, nhead = 1, num_encoder_layers = 1, num_decoder_layers = 1, dim_feedforward = 8, dropout_p = 0.1, layer_norm_eps = 1e-05)\n",
    "\n",
    "#print(model)\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters())} parameters ({sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9dc01f14-e1a4-453b-82fb-ee685412af9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 0 training loss 1.6703332662582397 (lr=0.01)\n",
      "   Epoch 100 training loss 0.4196763336658478 (lr=0.01)\n",
      "   Epoch 200 training loss 0.27427712082862854 (lr=0.01)\n",
      "   Epoch 300 training loss 0.20652401447296143 (lr=0.01)\n",
      "   Epoch 400 training loss 0.2573661506175995 (lr=0.01)\n",
      "   Epoch 500 training loss 0.2252040058374405 (lr=0.001)\n",
      "   Epoch 600 training loss 0.1909017413854599 (lr=0.001)\n",
      "   Epoch 700 training loss 0.20861311256885529 (lr=0.001)\n",
      "   Epoch 800 training loss 0.22771619260311127 (lr=0.001)\n",
      "   Epoch 900 training loss 0.2312566190958023 (lr=0.001)\n",
      "   Epoch 1000 training loss 0.23717661201953888 (lr=0.001)\n",
      "   Epoch 1100 training loss 0.17331108450889587 (lr=0.001)\n",
      "   Epoch 1200 training loss 0.23004981875419617 (lr=0.001)\n",
      "   Epoch 1300 training loss 0.21637175977230072 (lr=0.001)\n",
      "   Epoch 1400 training loss 0.20326192677021027 (lr=0.001)\n",
      "   Epoch 1500 training loss 0.18924254179000854 (lr=0.001)\n",
      "   Epoch 1600 training loss 0.2140977531671524 (lr=0.001)\n",
      "   Epoch 1700 training loss 0.18860164284706116 (lr=0.001)\n",
      "   Epoch 1800 training loss 0.17857539653778076 (lr=0.001)\n",
      "   Epoch 1900 training loss 0.19091862440109253 (lr=0.001)\n"
     ]
    }
   ],
   "source": [
    "num_of_epochs = 2000\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[500], gamma=0.1)\n",
    "model.train()\n",
    "for n in range(num_of_epochs):\n",
    "    running_loss = 0.0\n",
    "    X_in = X_tr.long()\n",
    "    Y_in = Y_tr[:-1,:].long()\n",
    "    Y_out = Y_tr[1:,:].long()\n",
    "\n",
    "    # Get mask to mask out the next words\n",
    "    sequence_length = Y_in.size(0)\n",
    "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(sequence_length)\n",
    "    \n",
    "    Y_pred = model(X_in,Y_in, tgt_mask = tgt_mask)\n",
    "\n",
    "    # seq len x num samples => num samples x seq len\n",
    "    Y_out = Y_out.permute(1,0)\n",
    "    # seq len x num samples x token one hot => num samples x token one hot x seq len    \n",
    "    Y_pred = Y_pred.permute(1, 2, 0)\n",
    "    loss = loss_fn(Y_pred,Y_out)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    if n % 100 == 0:\n",
    "        print(f'   Epoch {n} training loss {running_loss} (lr={optimizer.param_groups[0][\"lr\"]})')\n",
    "print(f'Final:   Epoch {n} training loss {running_loss} (lr={optimizer.param_groups[0][\"lr\"]})')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcbc1c45-b4e9-4ea0-be1e-4a40bb47466b",
   "metadata": {},
   "source": [
    "**Inference** must be updated as well for nn.Transformer since it re-produces all outputs sequentially, but without masking it \"sees\" the future that was not anymore allowed in training. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "498d0f25-7161-427e-aa50-94ce6863e1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_sequence, max_length=16, SOS_token=2, EOS_token=3, EOS_plus = 2):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    y_input = torch.tensor([SOS_token], dtype=torch.long)\n",
    "\n",
    "    sos_found = False\n",
    "    for _ in range(max_length):\n",
    "        #print(f'X={input_sequence} Y={y_input}')\n",
    "\n",
    "        sequence_length = len(y_input)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(sequence_length)\n",
    "    \n",
    "        pred = model(input_sequence, y_input, tgt_mask = tgt_mask)\n",
    "\n",
    "        pred_tokens = torch.argmax(pred, dim=1)\n",
    "\n",
    "        y_input = torch.cat((torch.tensor([SOS_token], dtype=torch.long), pred_tokens), dim=0)\n",
    "\n",
    "        if pred_tokens[-1] == EOS_token:\n",
    "            break\n",
    "\n",
    "    return y_input.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "a5c5d8f3-f3bb-4516-b682-0960a10802b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0\n",
      "Input: [1, 1, 1]\n",
      "Continuation: [0, 0, 0]\n",
      "\n",
      "Example 1\n",
      "Input: [0, 0, 0]\n",
      "Continuation: [1, 1, 1]\n",
      "\n",
      "Example 2\n",
      "Input: [1]\n",
      "Continuation: [0, 0, 0]\n",
      "\n",
      "Example 3\n",
      "Input: [0]\n",
      "Continuation: [1, 1, 1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we test some examples to observe how the model predicts\n",
    "examples = [\n",
    "    torch.tensor([2, 1, 1, 1, 3], dtype=torch.long),\n",
    "    torch.tensor([2, 0, 0, 0, 3], dtype=torch.long),\n",
    "    torch.tensor([2, 1, 3], dtype=torch.long),\n",
    "    torch.tensor([2, 0, 3], dtype=torch.long)\n",
    "]\n",
    "\n",
    "for idx, example in enumerate(examples):\n",
    "    result = predict(model, example)\n",
    "    print(f\"Example {idx}\")\n",
    "    print(f\"Input: {example.view(-1).tolist()[1:-1]}\")\n",
    "    print(f\"Continuation: {result[1:-1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cffde892-abee-410f-a663-e0228ffb5edf",
   "metadata": {},
   "source": [
    "Let's try something different - how about learning how to continue a sequence\n",
    "\n",
    "* sos,0,1,0,1,eos $\\rightarrow$ sos,0,1,0,1,eos\n",
    "* sos,1,0,1,0,eos $\\rightarrow$ sos,1,0,1,0,eos\n",
    "\n",
    "The above looks simple enough so that ```SimpleTransformerPlus```should be able to learn it now that it can process sequences correctly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "c176c421-0018-42fc-a4c0-abf94037554b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data4(n):\n",
    "    SOS_token = np.array([2])\n",
    "    EOS_token = np.array([3])\n",
    "\n",
    "    data = []\n",
    "\n",
    "    # 1 -> 0,0,0,0 \n",
    "    for i in range(n // 2):\n",
    "        X = np.concatenate((SOS_token, [0, 1, 0, 1], EOS_token))\n",
    "        y = np.concatenate((SOS_token, [0, 1, 0, 1], EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    # 1 -> 1,1,1,1 \n",
    "    for i in range(n // 2):\n",
    "        X = np.concatenate((SOS_token, [1, 0, 1, 0], EOS_token))\n",
    "        y = np.concatenate((SOS_token, [1, 0, 1, 0], EOS_token))\n",
    "        data.append([X, y])\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "b925b448-f20c-4c35-a38a-02a1a6cd7051",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 0 1 0 3]\n",
      "[2 1 0 1 0 3]\n",
      "[2 1 0 1 0 3]\n",
      "[2 1 0 1 0 3]\n",
      "[2 0 1 0 1 3]\n",
      "[2 0 1 0 1 3]\n"
     ]
    }
   ],
   "source": [
    "tr_data = generate_data4(100)\n",
    "\n",
    "print(tr_data[0][0])\n",
    "print(tr_data[0][1])\n",
    "\n",
    "print(tr_data[1][0])\n",
    "print(tr_data[1][1])\n",
    "\n",
    "print(tr_data[2][0])\n",
    "print(tr_data[2][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ce60c673-0b1f-405a-8c8d-d6901c81f4ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6, 100])\n",
      "tensor([2., 1., 0., 1., 0., 3.])\n",
      "tensor([2., 1., 0., 1., 0., 3.])\n",
      "tensor([2., 1., 0., 1., 0., 3.])\n",
      "tensor([2., 1., 0., 1., 0., 3.])\n",
      "tensor([2., 0., 1., 0., 1., 3.])\n",
      "tensor([2., 0., 1., 0., 1., 3.])\n"
     ]
    }
   ],
   "source": [
    "X_tr = torch.empty((len(tr_data[0][0]),len(tr_data)))\n",
    "Y_tr = torch.empty((len(tr_data[0][1]),len(tr_data)))\n",
    "for ids, s in enumerate(tr_data):\n",
    "    X_tr[:,ids] = torch.from_numpy(s[0])\n",
    "    Y_tr[:,ids] = torch.from_numpy(s[1])\n",
    "print(X_tr.shape)\n",
    "\n",
    "print(X_tr[:,0])\n",
    "print(Y_tr[:,0])\n",
    "\n",
    "print(X_tr[:,1])\n",
    "print(Y_tr[:,1])\n",
    "\n",
    "print(X_tr[:,2])\n",
    "print(Y_tr[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "289add26-c228-46b2-8bdf-5151e5953ffe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1332 parameters (1332 trainable)\n"
     ]
    }
   ],
   "source": [
    "model = MaskedTokenTransformer(num_tokens = 4, d_model = 8, nhead = 1, num_encoder_layers = 1, num_decoder_layers = 1, dim_feedforward = 8, dropout_p = 0.1, layer_norm_eps = 1e-05)\n",
    "\n",
    "#print(model)\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters())} parameters ({sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "e056bbb0-f801-4290-bf7b-07adcd961868",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 0 training loss 1.8352484703063965 (lr=0.01)\n",
      "   Epoch 100 training loss 0.45270711183547974 (lr=0.01)\n",
      "   Epoch 200 training loss 0.336453378200531 (lr=0.01)\n",
      "   Epoch 300 training loss 0.35191258788108826 (lr=0.01)\n",
      "   Epoch 400 training loss 0.23318374156951904 (lr=0.01)\n",
      "   Epoch 500 training loss 0.2831466495990753 (lr=0.001)\n",
      "   Epoch 600 training loss 0.29740801453590393 (lr=0.001)\n",
      "   Epoch 700 training loss 0.2862286865711212 (lr=0.001)\n",
      "   Epoch 800 training loss 0.3005932569503784 (lr=0.001)\n",
      "   Epoch 900 training loss 0.3319343328475952 (lr=0.001)\n",
      "   Epoch 1000 training loss 0.29023727774620056 (lr=0.001)\n",
      "   Epoch 1100 training loss 0.2839820683002472 (lr=0.001)\n",
      "   Epoch 1200 training loss 0.32187819480895996 (lr=0.001)\n",
      "   Epoch 1300 training loss 0.2752189040184021 (lr=0.001)\n",
      "   Epoch 1400 training loss 0.25834789872169495 (lr=0.001)\n",
      "   Epoch 1500 training loss 0.31446874141693115 (lr=0.001)\n",
      "   Epoch 1600 training loss 0.2911476492881775 (lr=0.001)\n",
      "   Epoch 1700 training loss 0.30333012342453003 (lr=0.001)\n",
      "   Epoch 1800 training loss 0.2911887466907501 (lr=0.001)\n",
      "   Epoch 1900 training loss 0.3157966136932373 (lr=0.001)\n",
      "Final:   Epoch 1999 training loss 0.2955414652824402 (lr=0.001)\n"
     ]
    }
   ],
   "source": [
    "num_of_epochs = 2000\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[500], gamma=0.1)\n",
    "model.train()\n",
    "for n in range(num_of_epochs):\n",
    "    running_loss = 0.0\n",
    "    X_in = X_tr.long()\n",
    "    Y_in = Y_tr[:-1,:].long()\n",
    "    Y_out = Y_tr[1:,:].long()\n",
    "\n",
    "    # Get mask to mask out the next words\n",
    "    sequence_length = Y_in.size(0)\n",
    "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(sequence_length)\n",
    "    \n",
    "    Y_pred = model(X_in,Y_in, tgt_mask = tgt_mask)\n",
    "\n",
    "    # seq len x num samples => num samples x seq len\n",
    "    Y_out = Y_out.permute(1,0)\n",
    "    # seq len x num samples x token one hot => num samples x token one hot x seq len    \n",
    "    Y_pred = Y_pred.permute(1, 2, 0)\n",
    "    loss = loss_fn(Y_pred,Y_out)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    if n % 100 == 0:\n",
    "        print(f'   Epoch {n} training loss {running_loss} (lr={optimizer.param_groups[0][\"lr\"]})')\n",
    "print(f'Final:   Epoch {n} training loss {running_loss} (lr={optimizer.param_groups[0][\"lr\"]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "dbb19d7a-6696-45df-9275-185847574c2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X=tensor([2, 0, 1, 0, 1, 3]) Y=tensor([2])\n",
      "X=tensor([2, 0, 1, 0, 1, 3]) Y=tensor([2, 1])\n",
      "X=tensor([2, 0, 1, 0, 1, 3]) Y=tensor([2, 1, 0])\n",
      "X=tensor([2, 0, 1, 0, 1, 3]) Y=tensor([2, 1, 0, 1])\n",
      "X=tensor([2, 0, 1, 0, 1, 3]) Y=tensor([2, 1, 0, 1, 0])\n",
      "Example 0\n",
      "Input: [0, 1, 0, 1]\n",
      "Continuation: [1, 0, 1, 0]\n",
      "\n",
      "X=tensor([2, 1, 0, 1, 0, 3]) Y=tensor([2])\n",
      "X=tensor([2, 1, 0, 1, 0, 3]) Y=tensor([2, 1])\n",
      "X=tensor([2, 1, 0, 1, 0, 3]) Y=tensor([2, 1, 0])\n",
      "X=tensor([2, 1, 0, 1, 0, 3]) Y=tensor([2, 1, 0, 1])\n",
      "X=tensor([2, 1, 0, 1, 0, 3]) Y=tensor([2, 1, 0, 1, 0])\n",
      "Example 1\n",
      "Input: [1, 0, 1, 0]\n",
      "Continuation: [1, 0, 1, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we test some examples to observe how the model predicts\n",
    "examples = [\n",
    "    torch.tensor([2, 0, 1, 0, 1, 3], dtype=torch.long),\n",
    "    torch.tensor([2, 1, 0, 1, 0, 3], dtype=torch.long),\n",
    "]\n",
    "\n",
    "for idx, example in enumerate(examples):\n",
    "    result = predict(model, example)\n",
    "    print(f\"Example {idx}\")\n",
    "    print(f\"Input: {example.view(-1).tolist()[1:-1]}\")\n",
    "    print(f\"Continuation: {result[1:-1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd177d52-f21a-49fc-bf8e-fc14c06ba497",
   "metadata": {},
   "source": [
    "**Findings**\n",
    "\n",
    "Despite of its power ```SimpleTransformer``` to predict future from past tokes, is not able to learn simple sequence continuation such as 0,1,0,1 -> 0,1,0,1 and 1,0,1,0 -> 1,0,1,0.\n",
    "\n",
    "The reason is that the embeddings for the tokens 0 and 1 are the same, and summing them up is the same for 0,1,0,1 and 1,0,1,0 (and even for 1100, 1001, 0011) so the transformer does not know what is the correct.\n",
    "\n",
    "In order to fix this problem we need to make Transformer aware about the position of each token. That can be done by\n",
    "\n",
    " * Positional encoding\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c3e8703-0c6d-4117-8626-c1ce7fc3da24",
   "metadata": {},
   "source": [
    "## Transformer - Positional encoding is the last missing ingredient\n",
    "\n",
    "Plethora of code snippets can be found for positional encoding so let's pick one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "228605ac-d123-4376-8791-df44cc011280",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    r\"\"\"Inject some information about the relative or absolute position of the tokens\n",
    "        in the sequence. The positional encodings have the same dimension as\n",
    "        the embeddings, so that the two can be summed. Here, we use sine and cosine\n",
    "        functions of different frequencies.\n",
    "    .. math::\n",
    "        \\text{PosEncoder}(pos, 2i) = sin(pos/10000^(2i/d_model))\n",
    "        \\text{PosEncoder}(pos, 2i+1) = cos(pos/10000^(2i/d_model))\n",
    "        \\text{where pos is the word position and i is the embed idx)\n",
    "    Args:\n",
    "        d_model: the embed dim (required).\n",
    "        dropout: the dropout value (default=0.1).\n",
    "        max_len: the max. length of the incoming sequence (default=5000).\n",
    "    Examples:\n",
    "        >>> pos_encoder = PositionalEncoding(d_model)\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        r\"\"\"Inputs of forward function\n",
    "        Args:\n",
    "            x: the sequence fed to the positional encoder model (required).\n",
    "        Shape:\n",
    "            x: [sequence length, batch size, embed dim]\n",
    "            output: [sequence length, batch size, embed dim]\n",
    "        Examples:\n",
    "            >>> output = pos_encoder(x)\n",
    "        \"\"\"\n",
    "\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60ebe7d4-e428-4d43-84c2-7fd29c12a8eb",
   "metadata": {},
   "source": [
    "The code shows that the encoding is additive so by inputting zero vectors we see how it works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "39f78502-1d1b-463f-832d-46121e6477e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "foo_x = torch.zeros((3,1,8))\n",
    "foo_encoding = PositionalEncoding(8)\n",
    "foo_x_enc = foo_encoding(foo_x)\n",
    "\n",
    "plt.plot(np.squeeze(foo_x_enc[0,0,:]).t(),'k-',label = '1')\n",
    "plt.plot(np.squeeze(foo_x_enc[1,0,:]).t(),'r--',label = '2')\n",
    "#plt.plot(np.squeeze(foo_x_enc[9,0,:]).t(),'g.-',label = '10')\n",
    "plt.legend()\n",
    "plt.savefig('posencoding.png',bbox_inches = 'tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "265feeff-dddf-407b-a2f5-3c767499cc02",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens,\n",
    "        d_model,\n",
    "        nhead,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dim_feedforward,\n",
    "        dropout_p,\n",
    "        layer_norm_eps\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        \n",
    "        # Token embedding layer - this takes care of converting integer to vectors\n",
    "        self.embedding = nn.Embedding(num_tokens, d_model)\n",
    "\n",
    "        # Token \"unembedding\" to one-hot token vector\n",
    "        self.unembedding = nn.Linear(d_model, num_tokens)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoder = PositionalEncoding(d_model=d_model, dropout=dropout_p)\n",
    "\n",
    "        # nn.Transformer that does the magic\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model = d_model,\n",
    "            nhead = nhead,\n",
    "            num_encoder_layers = num_encoder_layers,\n",
    "            num_decoder_layers = num_decoder_layers,\n",
    "            dim_feedforward = dim_feedforward,\n",
    "            dropout = dropout_p,\n",
    "            layer_norm_eps = layer_norm_eps,\n",
    "            norm_first = True\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src,\n",
    "        tgt,\n",
    "        tgt_mask = None\n",
    "    ):\n",
    "        # Note: src & tgt default size is (seq_length, batch_num, feat_dim)\n",
    "\n",
    "        # Token embedding\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "\n",
    "        # Positional encoding - this is sensitive that data _must_ be seq len x batch num x feat dim\n",
    "        # Inference often misses the batch num\n",
    "        if src.dim() == 2: # seq len x feat dim\n",
    "            src = torch.unsqueeze(src,1) \n",
    "        src = self.positional_encoder(src)\n",
    "        if tgt.dim() == 2: # seq len x feat dim\n",
    "            tgt = torch.unsqueeze(tgt,1) \n",
    "        tgt = self.positional_encoder(tgt)\n",
    "\n",
    "        # Transformer output\n",
    "        out = self.transformer(src, tgt, tgt_mask=tgt_mask)\n",
    "        out = self.unembedding(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "5ba01145-105a-4094-b491-bd713a90b208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1332 parameters (1332 trainable)\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(num_tokens = 4, d_model = 8, nhead = 1, num_encoder_layers = 1, num_decoder_layers = 1, dim_feedforward = 8, dropout_p = 0.1, layer_norm_eps = 1e-05)\n",
    "\n",
    "#print(model)\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters())} parameters ({sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "907a8f23-3fc3-4de5-9489-b7133deb4217",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 0 training loss 1.5990777015686035 (lr=0.01)\n",
      "   Epoch 100 training loss 0.3538847863674164 (lr=0.01)\n",
      "   Epoch 200 training loss 0.13715119659900665 (lr=0.01)\n",
      "   Epoch 300 training loss 0.09070254117250443 (lr=0.01)\n",
      "   Epoch 400 training loss 0.08589037507772446 (lr=0.01)\n",
      "   Epoch 500 training loss 0.09142681211233139 (lr=0.001)\n",
      "   Epoch 600 training loss 0.042587779462337494 (lr=0.001)\n",
      "   Epoch 700 training loss 0.03779209032654762 (lr=0.001)\n",
      "   Epoch 800 training loss 0.043141212314367294 (lr=0.001)\n",
      "   Epoch 900 training loss 0.051063988357782364 (lr=0.001)\n",
      "   Epoch 1000 training loss 0.030390389263629913 (lr=0.001)\n",
      "   Epoch 1100 training loss 0.058601852506399155 (lr=0.001)\n",
      "   Epoch 1200 training loss 0.025730988010764122 (lr=0.001)\n",
      "   Epoch 1300 training loss 0.0672808587551117 (lr=0.001)\n",
      "   Epoch 1400 training loss 0.045373789966106415 (lr=0.001)\n",
      "   Epoch 1500 training loss 0.024036342278122902 (lr=0.001)\n",
      "   Epoch 1600 training loss 0.030940517783164978 (lr=0.001)\n",
      "   Epoch 1700 training loss 0.03945839777588844 (lr=0.001)\n",
      "   Epoch 1800 training loss 0.04064814746379852 (lr=0.001)\n",
      "   Epoch 1900 training loss 0.017141984775662422 (lr=0.001)\n",
      "Final:   Epoch 1999 training loss 0.025800026953220367 (lr=0.001)\n"
     ]
    }
   ],
   "source": [
    "num_of_epochs = 2000\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[500], gamma=0.1)\n",
    "model.train()\n",
    "for n in range(num_of_epochs):\n",
    "    running_loss = 0.0\n",
    "    X_in = X_tr.long()\n",
    "    Y_in = Y_tr[:-1,:].long()\n",
    "    Y_out = Y_tr[1:,:].long()\n",
    "\n",
    "    # Get mask to mask out the next words\n",
    "    sequence_length = Y_in.size(0)\n",
    "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(sequence_length)\n",
    "    \n",
    "    Y_pred = model(X_in,Y_in, tgt_mask = tgt_mask)\n",
    "\n",
    "    # seq len x num samples => num samples x seq len\n",
    "    Y_out = Y_out.permute(1,0)\n",
    "    # seq len x num samples x token one hot => num samples x token one hot x seq len    \n",
    "    Y_pred = Y_pred.permute(1, 2, 0)\n",
    "    loss = loss_fn(Y_pred,Y_out)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    if n % 100 == 0:\n",
    "        print(f'   Epoch {n} training loss {running_loss} (lr={optimizer.param_groups[0][\"lr\"]})')\n",
    "print(f'Final:   Epoch {n} training loss {running_loss} (lr={optimizer.param_groups[0][\"lr\"]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 593,
   "id": "199ab909-818a-46ae-88ee-13352091ab62",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model, input_sequence, max_length=16, SOS_token=2, EOS_token=3, EOS_plus = 2):\n",
    "\n",
    "    model.eval()\n",
    "    \n",
    "    y_input = torch.tensor([SOS_token], dtype=torch.long)\n",
    "\n",
    "    sos_found = False\n",
    "    for _ in range(max_length):\n",
    "        #print(f'X={input_sequence} Y={y_input}')\n",
    "\n",
    "        sequence_length = len(y_input)\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(sequence_length)\n",
    "\n",
    "        pred = model(input_sequence, y_input, tgt_mask = tgt_mask)\n",
    "\n",
    "        # Since the positional encoding implementation the model returns seq len x batch num x feat num\n",
    "        if pred.dim() == 3:\n",
    "            pred = torch.squeeze(pred,1)\n",
    "        \n",
    "        pred_tokens = torch.argmax(pred, dim=1)\n",
    "\n",
    "        y_input = torch.cat((torch.tensor([SOS_token], dtype=torch.long), pred_tokens), dim=0)\n",
    "\n",
    "        if pred_tokens[-1] == EOS_token:\n",
    "            break\n",
    "\n",
    "    return y_input.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 595,
   "id": "2248cc82-1b41-4384-b7c1-6200bbea3e41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0\n",
      "Input: [0, 1, 0, 1]\n",
      "Continuation: [0, 1, 0, 1]\n",
      "\n",
      "Example 1\n",
      "Input: [1, 0, 1, 0]\n",
      "Continuation: [1, 0, 1, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we test some examples to observe how the model predicts\n",
    "examples = [\n",
    "    torch.tensor([2, 0, 1, 0, 1, 3], dtype=torch.long),\n",
    "    torch.tensor([2, 1, 0, 1, 0, 3], dtype=torch.long),\n",
    "]\n",
    "\n",
    "for idx, example in enumerate(examples):\n",
    "    result = predict(model, example)\n",
    "    print(f\"Example {idx}\")\n",
    "    print(f\"Input: {example.view(-1).tolist()[1:-1]}\")\n",
    "    print(f\"Continuation: {result[1:-1]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6baec2-4879-4e4e-974b-d52795d9836f",
   "metadata": {},
   "source": [
    "**Findings** Works like a charm!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fbce9c7-e300-40bb-b2a4-2201a7bbe957",
   "metadata": {},
   "source": [
    "## Padding\n",
    "\n",
    "How about learning all sequences alltogether - for that we need to use padding as the sequences are of different lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 596,
   "id": "3868fbd8-ad69-4412-924a-695394b36536",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_data5(n):\n",
    "    SOS_token = np.array([2])\n",
    "    EOS_token = np.array([3])\n",
    "\n",
    "    data = []\n",
    "    seq_len = []\n",
    "    \n",
    "    # 0,0,0,0 -> 0,0,0,0 \n",
    "    for i in range(n // 8):\n",
    "        X = np.concatenate((SOS_token, [0, 0, 0, 0], EOS_token))\n",
    "        y = np.concatenate((SOS_token, [0, 0, 0, 0], EOS_token))\n",
    "        data.append([X, y])\n",
    "        seq_len.append([4+2, 4+2])\n",
    "\n",
    "    # 1,1,1,1 -> 1,1,1,1 \n",
    "    for i in range(n // 8):\n",
    "        X = np.concatenate((SOS_token, [1, 1, 1, 1], EOS_token))\n",
    "        y = np.concatenate((SOS_token, [1, 1, 1, 1], EOS_token))\n",
    "        data.append([X, y])\n",
    "        seq_len.append([4+2, 4+2])\n",
    "\n",
    "    # 0,0,0 -> 1 \n",
    "    for i in range(n // 8):\n",
    "        X = np.concatenate((SOS_token, [0, 0, 0], EOS_token))\n",
    "        y = np.concatenate((SOS_token, [1], EOS_token))\n",
    "        data.append([X, y])\n",
    "        seq_len.append([3+2, 1+2])\n",
    " \n",
    "    # 1,1,1 -> 0 \n",
    "    for i in range(n // 8):\n",
    "        X = np.concatenate((SOS_token, [1, 1, 1], EOS_token))\n",
    "        y = np.concatenate((SOS_token, [0], EOS_token))\n",
    "        data.append([X, y])\n",
    "        seq_len.append([3+2, 1+2])\n",
    "\n",
    "    # 1 -> 0,0,0 \n",
    "    for i in range(n // 8):\n",
    "        X = np.concatenate((SOS_token, [1], EOS_token))\n",
    "        y = np.concatenate((SOS_token, [0, 0, 0], EOS_token))\n",
    "        data.append([X, y])\n",
    "        seq_len.append([1+2, 3+2])\n",
    "\n",
    "    # 0 -> 1,1,1 \n",
    "    for i in range(n // 8):\n",
    "        X = np.concatenate((SOS_token, [0], EOS_token))\n",
    "        y = np.concatenate((SOS_token, [1, 1, 1], EOS_token))\n",
    "        data.append([X, y])\n",
    "        seq_len.append([1+2, 3+2])\n",
    "\n",
    "    # 0,1,0,1 -> 0,1,0,1 \n",
    "    for i in range(n // 8):\n",
    "        X = np.concatenate((SOS_token, [0,1,0,1], EOS_token))\n",
    "        y = np.concatenate((SOS_token, [0,1,0,1], EOS_token))\n",
    "        data.append([X, y])\n",
    "        seq_len.append([4+2, 4+2])\n",
    "\n",
    "    # 1,0,1,0 -> 1,0,1,0 \n",
    "    for i in range(n // 8):\n",
    "        X = np.concatenate((SOS_token, [1,0,1,0], EOS_token))\n",
    "        y = np.concatenate((SOS_token, [1,0,1,0], EOS_token))\n",
    "        data.append([X, y])\n",
    "        seq_len.append([4+2, 4+2])\n",
    "\n",
    "    temp = list(zip(data, seq_len))  # Pair the elements\n",
    "    random.shuffle(temp)  # Shuffle the pairs\n",
    "    data, seq_len = zip(*temp)  # Unzip into separate lists\n",
    "\n",
    "    #np.random.shuffle(data)\n",
    "\n",
    "    return data, seq_len"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 597,
   "id": "4f740aae-eacf-4d50-821e-40fd6fec523a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 1 3]\n",
      "[2 0 0 0 3]\n",
      "[3, 5]\n",
      "[2 1 1 1 3]\n",
      "[2 0 3]\n",
      "[5, 3]\n",
      "[2 0 0 0 0 3]\n",
      "[2 0 0 0 0 3]\n",
      "[6, 6]\n"
     ]
    }
   ],
   "source": [
    "tr_data, tr_seq_len = generate_data5(200)\n",
    "\n",
    "print(tr_data[0][0])\n",
    "print(tr_data[0][1])\n",
    "print(tr_seq_len[0][:])\n",
    "\n",
    "print(tr_data[1][0])\n",
    "print(tr_data[1][1])\n",
    "print(tr_seq_len[1][:])\n",
    "\n",
    "print(tr_data[2][0])\n",
    "print(tr_data[2][1])\n",
    "print(tr_seq_len[2][:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 598,
   "id": "4fdacb0a-dd58-4390-8c5d-30df50240881",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6\n",
      "6\n",
      "torch.Size([6, 200])\n",
      "tensor([2., 1., 3., 4., 4., 4.])\n",
      "tensor([2., 0., 0., 0., 3., 4.])\n",
      "tensor([2., 1., 1., 1., 3., 4.])\n",
      "tensor([2., 0., 3., 4., 4., 4.])\n",
      "tensor([2., 0., 0., 0., 0., 3.])\n",
      "tensor([2., 0., 0., 0., 0., 3.])\n"
     ]
    }
   ],
   "source": [
    "PAD_IDX = 4\n",
    "max_len_X = max([foo[0] for foo in tr_seq_len])\n",
    "max_len_Y = max([foo[1] for foo in tr_seq_len])\n",
    "print(max_len_X)\n",
    "print(max_len_Y)\n",
    "\n",
    "X_tr = PAD_IDX*torch.ones((max_len_X,len(tr_data)))\n",
    "Y_tr = PAD_IDX*torch.ones((max_len_Y,len(tr_data)))\n",
    "for ids, s in enumerate(tr_data):\n",
    "    X_tr[:tr_seq_len[ids][0],ids] = torch.from_numpy(s[0])\n",
    "    Y_tr[:tr_seq_len[ids][1],ids] = torch.from_numpy(s[1])\n",
    "print(X_tr.shape)\n",
    "\n",
    "print(X_tr[:,0])\n",
    "print(Y_tr[:,0])\n",
    "\n",
    "print(X_tr[:,1])\n",
    "print(Y_tr[:,1])\n",
    "\n",
    "print(X_tr[:,2])\n",
    "print(Y_tr[:,2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 599,
   "id": "f3dabbf3-0f9f-4732-b051-b8de5e4a93d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False, False,  True,  True,  True])\n",
      "tensor([False, False, False, False, False,  True])\n",
      "tensor([False, False, False, False, False,  True])\n",
      "tensor([False, False, False,  True,  True,  True])\n",
      "tensor([False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False])\n"
     ]
    }
   ],
   "source": [
    "src_padding_mask = (X_tr == PAD_IDX).transpose(0, 1)\n",
    "tgt_padding_mask = (Y_tr == PAD_IDX).transpose(0, 1)\n",
    "\n",
    "print(src_padding_mask[0,:])\n",
    "print(tgt_padding_mask[0,:])\n",
    "print(src_padding_mask[1,:])\n",
    "print(tgt_padding_mask[1,:])\n",
    "print(src_padding_mask[2,:])\n",
    "print(tgt_padding_mask[2,:])\n",
    "\n",
    "# Own experiments\n",
    "#foo = src_padding_mask\n",
    "#print(foo.shape)\n",
    "#foo = foo.repeat(8,1,1)\n",
    "#print(foo.shape)\n",
    "#foo = foo.permute(2,1,0)\n",
    "#print(foo.shape)\n",
    "#print(foo[:,2,0])\n",
    "#foo = torch.where(foo == False, 1.0, 0.0)\n",
    "#print(foo[:,2,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 600,
   "id": "423a494b-5e63-4bf2-94f3-e576658f47a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(nn.Module):\n",
    "    # Constructor\n",
    "    def __init__(\n",
    "        self,\n",
    "        num_tokens,\n",
    "        d_model,\n",
    "        nhead,\n",
    "        num_encoder_layers,\n",
    "        num_decoder_layers,\n",
    "        dim_feedforward,\n",
    "        dropout_p,\n",
    "        layer_norm_eps,\n",
    "        padding_idx = None\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.padding_idx = padding_idx\n",
    "\n",
    "        if padding_idx != None:\n",
    "            # Token embedding layer - this takes care of converting integer to vectors\n",
    "            self.embedding = nn.Embedding(num_tokens+1, d_model, padding_idx = self.padding_idx)\n",
    "        else:\n",
    "            # Token embedding layer - this takes care of converting integer to vectors\n",
    "            self.embedding = nn.Embedding(num_tokens, d_model)\n",
    "        \n",
    "        # Token \"unembedding\" to one-hot token vector\n",
    "        self.unembedding = nn.Linear(d_model, num_tokens)\n",
    "\n",
    "        # Positional encoding\n",
    "        self.positional_encoder = PositionalEncoding(d_model=d_model, dropout=dropout_p)\n",
    "\n",
    "        # nn.Transformer that does the magic\n",
    "        self.transformer = nn.Transformer(\n",
    "            d_model = d_model,\n",
    "            nhead = nhead,\n",
    "            num_encoder_layers = num_encoder_layers,\n",
    "            num_decoder_layers = num_decoder_layers,\n",
    "            dim_feedforward = dim_feedforward,\n",
    "            dropout = dropout_p,\n",
    "            layer_norm_eps = layer_norm_eps,\n",
    "            norm_first = True\n",
    "        )\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        src,\n",
    "        tgt,\n",
    "        tgt_mask = None,\n",
    "        src_key_padding_mask = None,\n",
    "        tgt_key_padding_mask = None\n",
    "    ):\n",
    "        # Note: src & tgt default size is (seq_length, batch_num, feat_dim)\n",
    "\n",
    "        # Token embedding\n",
    "        src = self.embedding(src) * math.sqrt(self.d_model)\n",
    "        tgt = self.embedding(tgt) * math.sqrt(self.d_model)\n",
    "\n",
    "        # Positional encoding - this is sensitive that data _must_ be seq len x batch num x feat dim\n",
    "        # Inference often misses the batch num\n",
    "        if src.dim() == 2: # seq len x feat dim\n",
    "            src = torch.unsqueeze(src,1) \n",
    "        src = self.positional_encoder(src)\n",
    "        if tgt.dim() == 2: # seq len x feat dim\n",
    "            tgt = torch.unsqueeze(tgt,1) \n",
    "        tgt = self.positional_encoder(tgt)\n",
    "\n",
    "        # Transformer output\n",
    "        out = self.transformer(src, tgt, tgt_mask=tgt_mask, src_key_padding_mask = src_key_padding_mask,\n",
    "                               tgt_key_padding_mask=tgt_key_padding_mask, memory_key_padding_mask=src_key_padding_mask)\n",
    "        out = self.unembedding(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 602,
   "id": "cc62cbec-c394-4b14-ac90-b4572f1691fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model has 1340 parameters (1340 trainable)\n"
     ]
    }
   ],
   "source": [
    "model = Transformer(num_tokens = 4, d_model = 8, nhead = 1, num_encoder_layers = 1,\n",
    "                    num_decoder_layers = 1, dim_feedforward = 8, dropout_p = 0.1,\n",
    "                    layer_norm_eps = 1e-05, padding_idx = PAD_IDX)\n",
    "\n",
    "#print(model)\n",
    "print(f'The model has {sum(p.numel() for p in model.parameters())} parameters ({sum(p.numel() for p in model.parameters() if p.requires_grad)} trainable)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 603,
   "id": "f99f4367-84df-46c2-bd64-75a0047a49ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Epoch 0 training loss 1.5141552686691284 (lr=0.01)\n",
      "   Epoch 100 training loss 0.4676216244697571 (lr=0.01)\n",
      "   Epoch 200 training loss 0.3865887522697449 (lr=0.01)\n",
      "   Epoch 300 training loss 0.34180617332458496 (lr=0.01)\n",
      "   Epoch 400 training loss 0.2730879783630371 (lr=0.01)\n",
      "   Epoch 500 training loss 0.211541086435318 (lr=0.01)\n",
      "   Epoch 600 training loss 0.21127483248710632 (lr=0.01)\n",
      "   Epoch 700 training loss 0.16668538749217987 (lr=0.01)\n",
      "   Epoch 800 training loss 0.13057874143123627 (lr=0.01)\n",
      "   Epoch 900 training loss 0.1376904547214508 (lr=0.01)\n",
      "   Epoch 1000 training loss 0.11389752477407455 (lr=0.001)\n",
      "   Epoch 1100 training loss 0.12691161036491394 (lr=0.001)\n",
      "   Epoch 1200 training loss 0.1782669574022293 (lr=0.001)\n",
      "   Epoch 1300 training loss 0.13708484172821045 (lr=0.001)\n",
      "   Epoch 1400 training loss 0.1177263930439949 (lr=0.001)\n",
      "   Epoch 1500 training loss 0.15857520699501038 (lr=0.001)\n",
      "   Epoch 1600 training loss 0.1500292420387268 (lr=0.001)\n",
      "   Epoch 1700 training loss 0.12508845329284668 (lr=0.001)\n",
      "   Epoch 1800 training loss 0.10020900517702103 (lr=0.001)\n",
      "   Epoch 1900 training loss 0.1516122967004776 (lr=0.001)\n",
      "Final:   Epoch 1999 training loss 0.11666730046272278 (lr=0.001)\n"
     ]
    }
   ],
   "source": [
    "num_of_epochs = 2000\n",
    "loss_fn = torch.nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "scheduler = lr_scheduler.MultiStepLR(optimizer, milestones=[1000], gamma=0.1)\n",
    "model.train()\n",
    "for n in range(num_of_epochs):\n",
    "    running_loss = 0.0\n",
    "    X_in = X_tr.long()\n",
    "    Y_in = Y_tr[:-1,:].long()\n",
    "    Y_out = Y_tr[1:,:].long()\n",
    "    tgt_padding_mask_in = tgt_padding_mask[:,:-1]\n",
    "    \n",
    "    # Get mask to mask out the next words\n",
    "    sequence_length = Y_in.size(0)\n",
    "    tgt_mask = nn.Transformer.generate_square_subsequent_mask(sequence_length)\n",
    "    \n",
    "    Y_pred = model(X_in,Y_in, tgt_mask = tgt_mask, src_key_padding_mask = src_padding_mask,\n",
    "                   tgt_key_padding_mask = tgt_padding_mask_in)\n",
    "\n",
    "    # seq len x num samples => num samples x seq len\n",
    "    Y_out = Y_out.permute(1,0)\n",
    "    # seq len x num samples x token one hot => num samples x token one hot x seq len    \n",
    "    Y_pred = Y_pred.permute(1, 2, 0)\n",
    "    #print(Y_pred.shape)\n",
    "    loss = loss_fn(Y_pred,Y_out)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    running_loss += loss.item()\n",
    "    scheduler.step()\n",
    "    if n % 100 == 0:\n",
    "        print(f'   Epoch {n} training loss {running_loss} (lr={optimizer.param_groups[0][\"lr\"]})')\n",
    "print(f'Final:   Epoch {n} training loss {running_loss} (lr={optimizer.param_groups[0][\"lr\"]})')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 604,
   "id": "6a5307b5-6a2d-478a-ae77-4c4183a892aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example 0\n",
      "Input: [0, 0, 0, 0]\n",
      "Continuation: [0, 0, 0, 0]\n",
      "\n",
      "Example 1\n",
      "Input: [1, 1, 1, 1]\n",
      "Continuation: [1, 1, 1, 1]\n",
      "\n",
      "Example 2\n",
      "Input: [1, 1, 1]\n",
      "Continuation: [0]\n",
      "\n",
      "Example 3\n",
      "Input: [0, 0, 0]\n",
      "Continuation: [1]\n",
      "\n",
      "Example 4\n",
      "Input: [0]\n",
      "Continuation: [1, 1, 1]\n",
      "\n",
      "Example 5\n",
      "Input: [1]\n",
      "Continuation: [0, 0, 0]\n",
      "\n",
      "Example 6\n",
      "Input: [0, 1, 0, 1]\n",
      "Continuation: [0, 1, 0, 1]\n",
      "\n",
      "Example 7\n",
      "Input: [1, 0, 1, 0]\n",
      "Continuation: [1, 0, 1, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Here we test some examples to observe how the model predicts\n",
    "examples = [\n",
    "    torch.tensor([2, 0, 0, 0, 0, 3], dtype=torch.long),\n",
    "    torch.tensor([2, 1, 1, 1, 1, 3], dtype=torch.long),\n",
    "    torch.tensor([2, 1, 1, 1, 3], dtype=torch.long),\n",
    "    torch.tensor([2, 0, 0, 0, 3], dtype=torch.long),\n",
    "    torch.tensor([2, 0, 3], dtype=torch.long),\n",
    "    torch.tensor([2, 1, 3], dtype=torch.long),\n",
    "    torch.tensor([2, 0, 1, 0, 1, 3], dtype=torch.long),\n",
    "    torch.tensor([2, 1, 0, 1, 0, 3], dtype=torch.long),\n",
    "]\n",
    "\n",
    "for idx, example in enumerate(examples):\n",
    "    result = predict(model, example)\n",
    "    print(f\"Example {idx}\")\n",
    "    print(f\"Input: {example.view(-1).tolist()[1:-1]}\")\n",
    "    print(f\"Continuation: {result[1:-1]}\")\n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
